# 4:1 Multiplexer Verification (SystemVerilog)

This project implements a comprehensive verification environment for a **4:1 Multiplexer** using industry-standard SystemVerilog verification architecture. The testbench demonstrates professional verification practices with modular, reusable components including a driver, monitor, scoreboard, and transaction-based checking.

---

## ğŸ“Œ Design Specification â€” 4:1 Multiplexer

### Functional Behavior

The 4:1 MUX selects one of four 1-bit inputs based on a 2-bit select line and outputs the selected value:

| Select (S) | Output (Y) |
|:----------:|:----------:|
| `2'b00` | D0 |
| `2'b01` | D1 |
| `2'b10` | D2 |
| `2'b11` | D3 |

### Port Description

**Inputs:**
- `D0, D1, D2, D3` â†’ 1-bit data inputs
- `S[1:0]` â†’ 2-bit select line

**Outputs:**
- `Y` â†’ 1-bit multiplexed output

### Implementation
- **Logic Type**: Combinational
- **Design File**: `mux4to1.sv`

---

## ğŸ—ï¸ Verification Architecture

The verification environment follows a modular, component-based architecture inspired by industry-standard methodologies:

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   DRIVER    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   DUT    â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   MONITOR    â”‚
       â”‚ (Stimulus)  â”‚        â”‚ 4:1 MUX  â”‚        â”‚  (Capture)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â”‚ Transaction
                                                         â–¼
                                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚  SCOREBOARD      â”‚
                                                  â”‚  (Golden Model)  â”‚
                                                  â”‚  (Comparison)    â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Responsibilities

**Driver** (`driver.sv`)
- Generates random stimulus for all inputs (D0â€“D3, S)
- Applies inputs to DUT through the interface
- Maintains synchronization with simulation clock
- Enables controlled, repeatable test patterns

**Monitor** (`monitor.sv`)
- Passively observes DUT inputs and output
- Samples signals on every clock cycle
- Packages observations into transaction structures
- Feeds transactions to the scoreboard
- Does not modify DUT behavior

**Scoreboard** (`scoreboard.sv`)
- Computes expected output using golden reference model
- Compares expected vs actual DUT output
- Maintains per-cycle statistics and counters
- Logs PASS/ERROR messages with detailed information
- Provides final test summary

**Interface + Modports** (`mux_if.sv`)
- Encapsulates all DUT signals
- Provides separate perspectives via modports
- `dut_mp` â†’ For DUT instantiation
- `tb_mp` â†’ For testbench (driver/monitor) access
- Enables clean signal connectivity and type safety

**Program Block** (`test.sv`)
- Orchestrates driver, monitor, and scoreboard
- Coordinates stimulus generation and checking flow
- Manages simulation phases (reset, run, finish)
- Provides synchronized test execution

**Top Testbench** (`mux_tb.sv`)
- Instantiates interface, DUT, and program block
- Generates simulation clock
- Provides hierarchical top-level module

---

## ğŸ“ File Structure

```
mux_project/
â”‚
â”œâ”€â”€ mux4to1.sv           DUT â€” 4:1 multiplexer (combinational logic)
â”œâ”€â”€ mux_if.sv            Interface with modports (dut_mp, tb_mp)
â”œâ”€â”€ driver.sv            Generates and applies random stimulus
â”œâ”€â”€ monitor.sv           Samples and packages observations
â”œâ”€â”€ scoreboard.sv        Golden model and result checking
â”œâ”€â”€ test.sv              Program block (orchestration)
â””â”€â”€ mux_tb.sv            Top testbench (hierarchy)
```

**Design Principle**: Each file has a single, well-defined responsibility enabling modularity, reusability, and scalability.

---

## ğŸ“¦ Transaction Format

All observations are encapsulated in a standardized transaction struct for clean, type-safe communication:

```systemverilog
typedef struct {
    bit D0;              // Data input 0
    bit D1;              // Data input 1
    bit D2;              // Data input 2
    bit D3;              // Data input 3
    bit [1:0] S;         // Select line
    bit Y;               // Multiplexed output
} mux_trans_t;
```

**Data Flow**: Monitor samples â†’ Creates transaction â†’ Passes to Scoreboard

---

## ğŸ” Golden Reference Model

The scoreboard implements the expected behavior using a combinational model:

```systemverilog
case (tr.S)
    2'b00: expected = tr.D0;
    2'b01: expected = tr.D1;
    2'b10: expected = tr.D2;
    2'b11: expected = tr.D3;
endcase
```

**Verification Logic**:
```systemverilog
if (tr.Y !== expected) begin
    $display("[ERROR] S=%h | D=%b%b%b%b | Expected=%b | Got=%b", 
             tr.S, tr.D3, tr.D2, tr.D1, tr.D0, expected, tr.Y);
    error_count++;
end else begin
    $display("[PASS]  S=%h | D=%b%b%b%b | Y=%b", 
             tr.S, tr.D3, tr.D2, tr.D1, tr.D0, tr.Y);
    pass_count++;
end
```

---

## ğŸ§ª Simulation Flow

1. **Initialization** â€” Reset all components and DUT
2. **Stimulus Generation** â€” Driver creates random test vectors
3. **Application** â€” Driver applies D0â€“D3 and S to DUT inputs
4. **Observation** â€” Monitor samples all signals after propagation
5. **Prediction** â€” Scoreboard computes expected Y from golden model
6. **Verification** â€” Scoreboard compares expected vs actual
7. **Reporting** â€” Log PASS/FAIL with full transaction details
8. **Repetition** â€” Cycle repeats for configurable test iterations
9. **Summary** â€” Final statistics and test completion message

---

## ğŸ“¤ Example Simulation Output

```
===============================================
Starting 4:1 Multiplexer Verification...
===============================================

[PASS]  S=0 | D=1001 | Y=1
[PASS]  S=1 | D=0110 | Y=1
[PASS]  S=2 | D=0101 | Y=0
[PASS]  S=3 | D=1010 | Y=0
[PASS]  S=0 | D=1100 | Y=0
[PASS]  S=1 | D=0011 | Y=1
[PASS]  S=2 | D=1110 | Y=1
[PASS]  S=3 | D=0001 | Y=1
[PASS]  S=0 | D=1111 | Y=1
[PASS]  S=1 | D=0000 | Y=0

===============================================
Test Summary:
Total Tests:  10
PASSED:       10
FAILED:       0
===============================================
4:1 Multiplexer Verification Completed Successfully!
Simulation ended at 20 ns with $finish
```

---

## ğŸ› ï¸ Tool Setup & Execution

### Prerequisites
- **Simulator**: Vivado 2025.1 (XSIM)
- **Language**: SystemVerilog (IEEE 1800-2017)
- **OS**: Windows, Linux, or macOS

### Running the Simulation

1. **Create Project**
   ```
   Open Vivado â†’ Create New Project â†’ Select RTL
   ```

2. **Add Source Files**
   - Add all `.sv` files to Simulation Sources
   - Set file compilation order:
     1. `mux_if.sv` (interface first)
     2. `driver.sv`
     3. `monitor.sv`
     4. `scoreboard.sv`
     5. `test.sv`
     6. `mux4to1.sv`
     7. `mux_tb.sv`

3. **Configure Simulation**
   - Right-click on `mux_tb` â†’ Set as Top
   - Simulation Settings â†’ Set runtime (e.g., 100 ns)

4. **Run Simulation**
   ```
   Run Behavioral Simulation
   ```

5. **View Results**
   - Check Tcl Console for test output
   - Verify `[PASS]` messages for all transactions
   - Confirm final test summary

---

## ğŸ“ Verification Concepts Demonstrated

This project showcases professional verification practices:

âœ” **Interface Design** â€” Encapsulation of design signals using SystemVerilog interfaces  
âœ” **Modports** â€” Separate port sets for different component perspectives  
âœ” **Driver Component** â€” Controlled stimulus generation and synchronization  
âœ” **Monitor Component** â€” Non-intrusive observation and transaction packaging  
âœ” **Scoreboard Component** â€” Golden model implementation and automated checking  
âœ” **Transaction-Based Verification** â€” Structured data flow between components  
âœ” **Random Stimulus** â€” Systematic exploration of input space  
âœ” **Combinational Logic Verification** â€” Testing designs without sequential behavior  
âœ” **Modular Architecture** â€” Reusable, scalable verification components  
âœ” **UVM-Inspired Structure** â€” Industry-standard testbench organization  

---

## ğŸš€ Scalability & Future Enhancements

**Immediate Extensions:**
- Parameterize for N:1 multiplexers (any width, any number of inputs)
- Add weighted constrained randomization for targeted stimulus
- Implement functional coverage metrics

**Advanced Features:**
- Migrate to UVM methodology for enterprise-scale verification
- Add assertions (SVA) for continuous property checking
- Implement cross-coverage analysis
- Add performance and timing analysis
- Create self-checking test sequences

**Learning Path:**
- Master this 4:1 MUX baseline
- Apply patterns to larger combinational designs
- Extend to sequential designs (FIFOs, controllers, etc.)
- Graduate to UVM for real-world projects

---

## ğŸ“‹ Checklist â€” Verification Best Practices

This project implements several industry best practices:

- âœ… Clear component separation of concerns
- âœ… Transaction-based communication between components
- âœ… Golden reference model for expected behavior
- âœ… Comprehensive error reporting and diagnostics
- âœ… Type-safe data structures (structs, enums)
- âœ… Reusable, parameterizable interfaces
- âœ… Deterministic yet randomized stimulus
- âœ… Professional code organization and documentation
- âœ… Scalable architecture supporting multiple test iterations
- âœ… Clean final reporting and test summary

---

## ğŸ‰ Conclusion

This 4:1 Multiplexer verification project demonstrates a sophisticated, professional-grade verification environment scaled appropriately for the design complexity. By implementing driver, monitor, and scoreboard components with transaction-based communication, the project introduces industry-standard practices that form the foundation of enterprise-scale verification methodologies.

The modular architecture ensures that each component can be independently developed, tested, and reused. This clean separation of concerns makes the testbench maintainable, extensible, and serves as an excellent learning vehicle for understanding how larger verification frameworks are structured.

The techniques demonstrated hereâ€”interfaces, modports, random stimulus generation, golden models, and automated checkingâ€”are directly applicable to verifying real-world designs of any complexity, from simple combinational circuits to complex SoCs.

---

## ğŸ“š Key Takeaways

- Verification is as important as design
- Modular architecture scales with design complexity
- Golden models enable automated, objective checking
- Transaction-based flow simplifies component integration
- Random stimulus provides comprehensive coverage
- Professional organization enables team collaboration

---

**Thank you for exploring this verification project!**  
Use this as a reference, adapt it to your designs, or extend it with advanced features. Happy verifying! ğŸ¯
